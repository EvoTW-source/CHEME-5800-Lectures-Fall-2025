{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f298a06-6f24-4090-8778-0eadd4729806",
   "metadata": {},
   "source": [
    "# Example: Feature Hashing of Sarcasm Samples\n",
    "Another strategy to represent text in mathematical form is [feature-hashing](https://en.wikipedia.org/wiki/Feature_hashing). \n",
    "\n",
    "Feature Hashing is a technique used to convert text into a fixed-size numerical representation, __without__ the need for an explicit vocabulary. Let's look at a specific algorithm, the __Weinberger Feature Hashing Algorithm__. This is also known as the __hashing trick__. This approach does __not__ require an explicit vocabulary $\\mathcal{V}$. Thus, it can handle large vocabularies and unseen words gracefully.\n",
    "\n",
    "__Initialization:__ Given an array of tokens $\\mathbf{X} = \\{x_1, x_2, \\ldots, x_n\\}$, where $x_{i}\\in\\mathcal{V}$, and a dimension $d$, initialize a result array $\\mathbf R = \\mathbf 0\\in\\mathbb R^d.$\n",
    "\n",
    "\n",
    "For each $x\\in\\mathbf{X}$ __do__:\n",
    "1. Compute the hash value of the current token: $h \\gets\\texttt{hash}(x)$.\n",
    "2. Compute the index of the hash value in the result array: $i \\gets h \\mod d$.\n",
    "3. Update the result array: $\\mathbf{R}_{i} \\gets \\mathbf{R}_{i} + 1$.\n",
    "\n",
    "> **Note (Weinberger sign variant):**  \n",
    "> Optionally use a sign function $s(x)\\in\\{+1,-1\\}$ (e.g., low bit of $h$) so that  \n",
    "> $$\\mathbf R_i \\;\\mathrel{+}= s(x),$$  \n",
    "> which helps decorrelate hash collisions.\n",
    "\n",
    "\n",
    "**Example**  \n",
    "```text\n",
    "Tokens: [\"Hello\", \"world!\", \"This\", \"is\", \"a\", \"test\", \".\"]\n",
    "d = 10\n",
    "\n",
    "Possible output:\n",
    "[0, 1, 4, 0, 2, 1, 0, 1, 1, 0]\n",
    "```\n",
    "\n",
    "### Learning objectives\n",
    "This example will familiarize students with using [the Weinberger feature hashing algorithm](https://en.wikipedia.org/wiki/Feature_hashing) to compute high-dimensional vectors representing unstructured text. The tasks for this example are:\n",
    "* __Task 1: Prerequisites.__ To save some time, we'll load the saved file from the `SarcasmSamplesTokenizer` example using [the `load(...)` method exported by the FileIO.jl package](https://github.com/JuliaIO/FileIO.jl). We can then pull out some stuff we computed last time and reuse it here.\n",
    "* __Task 2: Explore Feature Hashing.__: Compute the feature hash vectors for the sarcastic samples. In this task, we'll compute [the feature hash vector representation](https://en.wikipedia.org/wiki/Feature_hashing) of the text headline for each sarcastic sample.\n",
    "\n",
    "Let's get started!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9500dca4-1aa5-4b35-ad3f-942212a96a32",
   "metadata": {},
   "source": [
    "## Task 1: Setup, Data, and Prerequisites\n",
    "First, we set up the computational environment by including the `Include.jl` file and loading any needed resources.\n",
    "* The [include command](https://docs.julialang.org/en/v1/base/base/#include) evaluates the contents of the input source file, `Include.jl`, in the notebook's global scope. The `Include.jl` file sets paths, loads required external packages, etc. For additional information on functions and types used in this material, see the [Julia programming language documentation](https://docs.julialang.org/en/v1/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7ab154de-1a83-4a53-a419-7a9adfe8c10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Include.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a9764e",
   "metadata": {},
   "source": [
    "In addition to standard Julia libraries, we'll also use [the `VLDataScienceMachineLearningPackage.jl` package](https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl), check out [the documentation](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/) for more information on the functions, types and data used in this material. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c1986b-3c5c-4780-a167-9ac1608f9a63",
   "metadata": {},
   "source": [
    "### Data\n",
    "To save some time, we'll load the saved file from the `SarcasmSamplesTokenizer` example using [the `load(...)` method exported by the FileIO.jl package](https://github.com/JuliaIO/FileIO.jl). To load the `jld2` (binary) saved file, we pass the path to the file we want to load the [`load(...)` function](https://github.com/JuliaIO/FileIO.jl). This call returns the data as a [Julia `Dict` type](https://docs.julialang.org/en/v1/base/collections/#Base.Dict). \n",
    "\n",
    "Set the path to the save file in the `path_to_save_file::String` variable. Then load the `jld2` file using [the `load(...)` method](https://juliaio.github.io/FileIO.jl/stable/reference/#FileIO.load), where the contents of the file are stored in the `saved_data_dictionary::Dict{String, Any}` variable. \n",
    "\n",
    "We saved the `corpusmodel::MySarcasmRecordCorpusModel` instance, which holds the other interesting data, e.g., the `tokendictionary.` Thus, we can get (most) of everything we need from the `corpusmodel.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "74be06da-ef39-4af5-93b2-bc70587e58f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save_file = joinpath(_PATH_TO_DATA, \"CHEME-141-M4-SarcasmSamplesTokenizer-SavedData.jld2\"); # JLD2 package encodes data\n",
    "saved_data_dictionary = load(path_to_save_file);\n",
    "corpusmodel = saved_data_dictionary[\"corpus\"]; # pull data from the saved_data_dictionary -\n",
    "list_of_records = corpusmodel.records; # the list of records from the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3f9934",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decd2257-cbe3-4b5e-98a7-9c1853bd9e90",
   "metadata": {},
   "source": [
    "## Task 2: Compute the feature hash vectors for sarcastic samples\n",
    "In this task, we'll compute the feature hash vector representation of the text headlines for each __sarcastic sample__ in our sarcasm dataset. \n",
    "\n",
    "First, we need to collect all the sarcastic samples from the `corpusmodel::MySarcasmRecordCorpusModel` we loaded from the previous example. Let's interate through the records, and grab those that have the `is_sarcastic` flag set to `true`.\n",
    "\n",
    "> __Record model__: The `corpusmodel.records` are [instances of the `MySarcasmRecordModel` type](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/types/#VLDataScienceMachineLearningPackage.MySarcasmRecordModel). Each record model has the following fields:\n",
    ">    * `issarcastic`: has a value of `1` if the record is sarcastic; otherwise, `0.`\n",
    ">    * `headline`: the headline of the article, unstructured text\n",
    ">    * `article_link`: link to the original news article. Useful in collecting supplementary data\n",
    "\n",
    "Let's save the sarcastic samples in the `my_sarcastic_samples::Vector{MySarcasmRecordModel}` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e79ff0ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13634-element Vector{MySarcasmRecordModel}:\n",
       " MySarcasmRecordModel(true, \"author dismayed by amazon customers other purchases\", \"https://www.theonion.com/author-dismayed-by-amazon-customers-other-purchases-1819567854\")\n",
       " MySarcasmRecordModel(true, \"dollhousing crisis set to worsen mean older brother says\", \"https://entertainment.theonion.com/doll-housing-crisis-set-to-worsen-mean-older-brother-s-1819569425\")\n",
       " MySarcasmRecordModel(true, \"fear notshe means you no harm says elizabeth warren revealing docile hillary clinton to crowd\", \"https://politics.theonion.com/fear-not-she-means-you-no-harm-says-elizabeth-warren-1819579041\")\n",
       " MySarcasmRecordModel(true, \"clinton credits nevada victory to inescapable pitchblack tide of fate\", \"https://politics.theonion.com/clinton-credits-nevada-victory-to-inescapable-pitch-bl-1819578631\")\n",
       " MySarcasmRecordModel(true, \"merger of advertising giants brings together largest collection of people with no discernible skills\", \"https://www.theonion.com/merger-of-advertising-giants-brings-together-largest-co-1819575330\")\n",
       " MySarcasmRecordModel(true, \"bartender refuses to acknowledge patrons regular status\", \"https://www.theonion.com/bartender-refuses-to-acknowledge-patrons-regular-status-1819567110\")\n",
       " MySarcasmRecordModel(true, \"adolescent girl reaching age where she starts exploring stepfathers body\", \"https://www.theonion.com/adolescent-girl-reaching-age-where-she-starts-exploring-1819575442\")\n",
       " MySarcasmRecordModel(true, \"mrs butterworths bottle central to terrifying lsd experience\", \"https://www.theonion.com/mrs-butterworths-bottle-central-to-terrifying-lsd-expe-1819565203\")\n",
       " MySarcasmRecordModel(true, \"either someone 14th caller or everything on fire at spanish radio station\", \"https://www.theonion.com/either-someone-14th-caller-or-everything-on-fire-at-spa-1819570416\")\n",
       " MySarcasmRecordModel(true, \"arsenio hall writers still keeping in touch\", \"https://entertainment.theonion.com/arsenio-hall-writers-still-keeping-in-touch-1819565840\")\n",
       " ⋮\n",
       " MySarcasmRecordModel(true, \"trump announces paris climate deal rejection in front of 16 running faucets\", \"https://politics.theonion.com/trump-announces-paris-climate-deal-rejection-in-front-o-1819592833\")\n",
       " MySarcasmRecordModel(true, \"brita unveils new inthroat water filters\", \"https://www.theonion.com/brita-unveils-new-in-throat-water-filters-1819578824\")\n",
       " MySarcasmRecordModel(true, \"fred durst spray paints limp bizkit on bridge\", \"https://entertainment.theonion.com/fred-durst-spray-paints-limp-bizkit-on-bridge-1819589561\")\n",
       " MySarcasmRecordModel(true, \"34yearold asks for big piece\", \"https://local.theonion.com/34-year-old-asks-for-big-piece-1819579465\")\n",
       " MySarcasmRecordModel(true, \"man always makes sure to put phone on silent before misplacing it\", \"https://local.theonion.com/man-always-makes-sure-to-put-phone-on-silent-before-mis-1832702681\")\n",
       " MySarcasmRecordModel(true, \"dream team wins small soft drink\", \"https://www.theonion.com/dream-team-wins-small-soft-drink-1819563970\")\n",
       " MySarcasmRecordModel(true, \"blindfolded clinton invites debate coaches to attack her with talking points from all sides\", \"https://politics.theonion.com/blindfolded-clinton-invites-debate-coaches-to-attack-he-1819579258\")\n",
       " MySarcasmRecordModel(true, \"epa unveils plan to improve conditions for nations sludge\", \"https://politics.theonion.com/epa-unveils-plan-to-improve-conditions-for-nation-s-slu-1819580020\")\n",
       " MySarcasmRecordModel(true, \"50 million worth of diamonds stolen in average day in brussels\", \"https://www.theonion.com/50-million-worth-of-diamonds-stolen-in-average-day-in-1819574573\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_sarcastic_samples = let\n",
    "\n",
    "    # initialize -\n",
    "    records = Vector{MySarcasmRecordModel}(); # we use an empty vector to hold the sarcastic samples\n",
    "\n",
    "    # proccess each record in the list of records -\n",
    "    for (k,v) ∈ list_of_records\n",
    "        if v.issarcastic == 1 # check: is the record sarcastic?\n",
    "            push!(records, v); # if yes, grab it\n",
    "        end\n",
    "    end\n",
    "    records;\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c380741",
   "metadata": {},
   "source": [
    "To see what is going on with feature hashing, let's grab a random sarcastic sample from our dataset and compute its feature hash vector representation.\n",
    "\n",
    "> __Idea__: To better understand how this works, let's first examine a single (random) record and compute the feature hash for it.  We'll select a random record from the `number_of_records::Int64` possible records [using the built-in `rand(...)` method](https://docs.julialang.org/en/v1/stdlib/Random/#Base.rand), and store it in the `random_test_record::MySarcasmRecordModel` variable\n",
    "\n",
    "\n",
    "Select a random record:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fc0ea0a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MySarcasmRecordModel(true, \"new ted cruz attack ad declares beto orourke too good for texas\", \"https://politics.theonion.com/new-ted-cruz-attack-ad-declares-beto-o-rourke-too-good-1829842240\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_test_record = let \n",
    "    number_of_records = length(my_sarcastic_samples); # how many sarcastic records do we have?\n",
    "    random_index = rand(1:number_of_records);          # pick a random index\n",
    "    my_sarcastic_samples[random_index]                  # get the record at that index\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0727747",
   "metadata": {},
   "source": [
    "Next, let's chop up the headline text into tokens, and compute the feature hash vector for the random record. We'll start by __tokenizing__ the `headline` field of the `random_test_record` variable.\n",
    "> __Tokenization:__ breaks down a string of text into smaller components, called tokens. In this case, we will tokenize the `headline` field of the `random_test_record` variable into an array of words by using [the split(...) method](https://docs.julialang.org/en/v1/base/strings/#Base.split) to split the `headline` string by cutting at the space characters. \n",
    "\n",
    "Let's save the array of tokens (words) in the `tokens::Vector{String}` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2adb1af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12-element Vector{String}:\n",
       " \"new\"\n",
       " \"ted\"\n",
       " \"cruz\"\n",
       " \"attack\"\n",
       " \"ad\"\n",
       " \"declares\"\n",
       " \"beto\"\n",
       " \"orourke\"\n",
       " \"too\"\n",
       " \"good\"\n",
       " \"for\"\n",
       " \"texas\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokens = split(random_test_record.headline, \" \") .|> String # tokenize the headline text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92aa18a",
   "metadata": {},
   "source": [
    "Now, we compute the feature hash vector for the `random_test_record` variable using [the `featurehashing(...)` method](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/text/#VLDataScienceMachineLearningPackage.featurehashing). This method takes the `tokens::Vector{String}` variable, a length that specifies the size of the output vector, and an algorithm parameter (signed or unsigned). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f4796d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20×2 Matrix{Int64}:\n",
       " 0   0\n",
       " 0   0\n",
       " 2   2\n",
       " 1  -1\n",
       " 2   2\n",
       " 1  -1\n",
       " 1   1\n",
       " 0   0\n",
       " 0   0\n",
       " 0   0\n",
       " 1   1\n",
       " 0   0\n",
       " 0   0\n",
       " 0   0\n",
       " 1   1\n",
       " 0   0\n",
       " 0   0\n",
       " 0   0\n",
       " 2   2\n",
       " 1  -1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_hash_vectors = let\n",
    "\n",
    "    # initialize -\n",
    "    d = 20; # size of the output vector\n",
    "\n",
    "    # compute the vectors -\n",
    "    v₁ = featurehashing(tokens, d = d, algorithm = UnsignedFeatureHashing()); # unsigned feature hashing\n",
    "    v₂ = featurehashing(tokens, d = d, algorithm = SignedFeatureHashing()); # signed feature hashing\n",
    "\n",
    "    [v₁ v₂] # return: [unsigned signed]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93afbb0",
   "metadata": {},
   "source": [
    "The `example_hash_vectors` array shows two different feature hash representations of our random headline, displayed side-by-side for comparison. The first column shows the basic Weinberger feature hashing algorithm where each token increments its corresponding hash bucket by +1, while the second column shows the signed variant where tokens can either add +1 or -1 to their hash bucket, depending on the sign function.\n",
    "\n",
    "#### Key Differences from Tokenization (M4 vs. Previous Tokenizer Example)\n",
    "\n",
    "The `d` parameter controls the fixed dimensionality of our feature vectors, which is fundamentally different from what we did in the tokenizer notebook. Unlike the tokenizer where vocabulary size was determined by the dataset (resulting in around 29,000+ unique tokens), here we choose `d=20` to create exactly 20-dimensional vectors regardless of how many unique words exist in our text.\n",
    "\n",
    "This represents a completely different philosophy for text representation. Feature hashing doesn't require building or storing a vocabulary dictionary—each word is directly mapped to a position using a hash function, not a lookup table. Every text sample produces the same-sized vector (d dimensions), regardless of text length or vocabulary diversity, whereas the tokenizer produced variable-length sequences that required padding.\n",
    "\n",
    "The trade-off here is that multiple different words can map to the same position, which we call a hash collision. This is a controlled compromise where we accept some information loss for computational efficiency and memory savings. Unknown words are handled automatically through hashing, without needing special `<unk>` tokens like we used in the tokenization approach.\n",
    "\n",
    "The signed variant helps reduce the impact of these hash collisions. When two words map to the same bucket, instead of both adding +1 (which could artificially inflate that feature's importance), they might add +1 and -1, potentially canceling each other out and reducing spurious correlations.\n",
    "\n",
    "This approach trades some precision for significant computational advantages, especially when dealing with large vocabularies or streaming text where building explicit vocabularies is impractical."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.6",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
